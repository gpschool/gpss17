{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab session 1: Gaussian Process models with GPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Process Summer School, 11th Semptember 2017\n",
    "\n",
    "written by Nicolas Durrande, Neil Lawrence and James Hensman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this lab session is to illustrate the concepts seen during the lectures. We will focus on three aspects of GPs: the kernel, the random sample paths and the GP regression model.\n",
    "\n",
    "Since the background of the attendees is very diverse, this session will attempt to cover a large spectrum from the very basic of GPs to more technical questions. This in mind, the difficulties of the questions of the lab session varies. We do not mark these labs in any way, you should focus on the parts that you find most useful. **In this session you should complete at least as far as Exercise 5**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Getting started: The Covariance Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that GPy is already installed on your machine. You can get instructions on how to install GPy from the  [SheffieldML github page](https://github.com/SheffieldML/GPy).\n",
    "\n",
    "We first tell the jupyter notebook that we want the plots to appear inline, then we import the libraries we will need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import GPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current draft of the online documentation of GPy is available from [this page](http://gpy.readthedocs.org/en/latest/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with defining an exponentiated quadratic covariance function (also known as squared exponential or rbf or Gaussian) in one dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = 1          # input dimension\n",
    "var = 1.       # variance\n",
    "theta = 0.2    # lengthscale\n",
    "k = GPy.kern.RBF(d, variance=var, lengthscale=theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A summary of the kernel can be obtained using the command `print k`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to plot the kernel as a function of one of its inputs (whilst fixing the other) with `k.plot()`. \n",
    "\n",
    "*Note*: if you need help with a command in jupyter notebook, then you can get it at any time by typing a question mark after the command, e.g. `k.plot?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Covariance Function Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of the covariance function parameters can be accessed and modified using `k['.*var']` where the string in bracket is a regular expression matching the parameter name as it appears in `print k`. Let's use this to get an insight into the effect of the parameters on the shape of the covariance function. \n",
    "\n",
    "We'll now use to set the lengthscale of the covariance to different values, and then plot the resulting covariance using the `k.plot()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = GPy.kern.RBF(d)     # By default, the parameters are set to 1.\n",
    "theta = np.asarray([0.2,0.5,1.,2.,4.])\n",
    "for t in theta:\n",
    "    k.lengthscale=t\n",
    "    k.plot()\n",
    "plt.legend(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) What is the effect of the lengthscale parameter on the covariance function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Now change the code used above for plotting the covariances associated with the length scale to see the influence of the variance parameter. What is the effect of the the variance parameter on the covariance function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance Functions in GPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many covariance functions are already implemented in GPy. Instead of rbf, try constructing and plotting the following  covariance functions: `exponential`, `Matern32`, `Matern52`, `Brownian`, `linear`, `bias`,\n",
    "`rbfcos`, `periodic_Matern32`, etc. Some of these covariance functions, such as `rbfcos`, are not\n",
    "parametrized by a variance and a lengthscale. Furthermore, not all kernels are stationary (i.e., they can’t all be written as $k ( x, y) = f ( x − y)$, see for example the Brownian\n",
    "covariance function). For plotting  so it may be interesting to change the value of the fixed input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb = GPy.kern.Brownian(input_dim=1)\n",
    "inputs = np.array([2., 4.])\n",
    "for x in inputs:\n",
    "    kb.plot(x,plot_limits=[0,5])\n",
    "plt.legend(inputs)\n",
    "plt.ylim(-0.1,5.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Covariance Function given the Input Data, $\\mathbf{X}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\mathbf{X}$ be a $n$ × $d$ numpy array. Given a kernel $k$, the covariance matrix associated to\n",
    "$\\mathbf{X}$ is obtained with `C = k.K(X,X)` . The positive semi-definiteness of $k$ ensures that `C`\n",
    "is a positive semi-definite (psd) matrix regardless of the initial points $\\mathbf{X}$. This can be\n",
    "checked numerically by looking at the eigenvalues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = GPy.kern.Matern52(input_dim=2)\n",
    "X = np.random.rand(50,2)       # 50*2 matrix of iid standard Gaussians\n",
    "C = k.K(X,X)\n",
    "eigvals = np.linalg.eigvals(C)           # Computes the eigenvalues of a matrix\n",
    "plt.bar(np.arange(len(eigvals)), eigvals)\n",
    "plt.title('Eigenvalues of the Matern 5/2 Covariance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Covariance Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) A matrix, $\\mathbf{K}$, is positive semi-definite if the matrix inner product, $\\mathbf{x}^\\top \\mathbf{K}\\mathbf{x}$ is greater than or equal to zero regardless of the values in $\\mathbf{x}$. Given this it should be easy to see that the sum of two positive semi-definite matrices is also positive semi-definite. In the context of Gaussian processes, this is the sum of two covariance functions. What does this mean from a modelling perspective? "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# exercise 2 a) answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Hint*: there are actually two related interpretations for this. Think about the properties of a Gaussian distribution, and where the sum of Gaussian variances arises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the element-wise product of two covariance functions? In other words if we define"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "k(\\mathbf{x}, \\mathbf{x}^\\prime) = k_1(\\mathbf{x}, \\mathbf{x}^\\prime) k_2(\\mathbf{x}, \\mathbf{x}^\\prime)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then is $k(\\mathbf{x}, \\mathbf{x}^\\prime)$ a valid covariance function?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Exercise 2 b) answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Covariance Functions in GPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In GPy you can easily combine covariance functions you have created using the sum and product operators, `+` and `*`. So, for example, if we wish to combine an exponentiated quadratic covariance with a Matern 5/2 then we can write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kern1 = GPy.kern.RBF(1, variance=1., lengthscale=2.)\n",
    "kern2 = GPy.kern.Matern52(1, variance=2., lengthscale=4.)\n",
    "kern = kern1 + kern2\n",
    "print(kern)\n",
    "kern.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or if we wanted to multiply them we can write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kern = kern1*kern2\n",
    "print(kern)\n",
    "kern.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Sampling from a Gaussian Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gaussian process provides a prior over an infinite dimensional function. It is defined by a covariance *function* and a mean *function*. When we compute the covariance matrix using `kern.K(X, X)` we are computing a covariance *matrix* between the values of the function that correspond to the input locations in the matrix `X`. If we want to have a look at the type of functions that arise from a particular Gaussian process we can never generate all values of the function, because there are infinite values. However, we can generate samples from a Gaussian *distribution* based on a covariance matrix associated with a particular matrix of input locations `X`. If these locations are chosen appropriately then they give us a good idea of the underlying function. For example, for a one dimensional function, if we choose `X` to be uniformly spaced across part of the real line, and the spacing is small enough, we'll get an idea of the underlying function. We will now use this trick to draw sample paths from a Gaussian process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = GPy.kern.RBF(input_dim=1,lengthscale=0.2)\n",
    "\n",
    "X = np.linspace(0.,1.,500) # define X to be 500 points evenly spaced over [0,1]\n",
    "X = X[:,None] # reshape X to make it n*p --- we try to use 'design matrices' in GPy \n",
    "\n",
    "mu = np.zeros((500)) # vector of the means --- we could use a mean function here, but here it is just zero.\n",
    "C = k.K(X,X) # compute the covariance matrix associated with inputs X\n",
    "\n",
    "# Generate 20 separate samples paths from a Gaussian with mean mu and covariance C\n",
    "Z = np.random.multivariate_normal(mu,C,20)\n",
    "\n",
    "plt.figure()     # open a new plotting window\n",
    "for i in range(20):\n",
    "    plt.plot(X[:],Z[i,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our choice of `X` means that the points are close enough together to look like functions. We can see the structure of the covariance matrix we are plotting from if we visualize C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try a range of different covariance functions and values and plot the corresponding sample paths for each using the same approach given above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try plotting sample paths here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you tell the covariance structures that have been used for generating the\n",
    "sample paths shown in the figure below?\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"http://ml.dcs.shef.ac.uk/gpss/gpws14/figa.png\" alt=\"Figure a\" style=\"width: 30%;\"> \n",
    "<img src=\"http://ml.dcs.shef.ac.uk/gpss/gpws14/figb.png\" alt=\"Figure b\" style=\"width: 30%;\">\n",
    "<img src=\"http://ml.dcs.shef.ac.uk/gpss/gpws14/figc.png\" alt=\"Figure c\" style=\"width: 30%;\"> \n",
    "<img src=\"http://ml.dcs.shef.ac.uk/gpss/gpws14/figd.png\" alt=\"Figure d\" style=\"width: 30%;\">\n",
    "<img src=\"http://ml.dcs.shef.ac.uk/gpss/gpws14/fige.png\" alt=\"Figure e\" style=\"width: 30%;\"> \n",
    "<img src=\"http://ml.dcs.shef.ac.uk/gpss/gpws14/figf.png\" alt=\"Figure f\" style=\"width: 30%;\">\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Exercise 3 answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 A Gaussian Process Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now combine the Gaussian process prior with some data to form a GP regression model with GPy. We will generate data from the function $f ( x ) = − \\cos(\\pi x ) + \\sin(4\\pi x )$ over $[0, 1]$, adding some noise to give $y(x) = f(x) + \\epsilon$, with the noise being Gaussian distributed, $\\epsilon \\sim \\mathcal{N}(0, 0.01)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.normal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(0.05,0.95,10)[:,None]\n",
    "Y = -np.cos(np.pi*X) + np.sin(4*np.pi*X) + np.random.normal(loc=0.0, scale=0.1, size=(10,1)) \n",
    "plt.figure()\n",
    "plt.plot(X,Y,'kx',mew=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A GP regression model based on an exponentiated quadratic covariance function can be defined by first defining a covariance function, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = GPy.kern.RBF(input_dim=1, variance=1., lengthscale=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then combining it with the data to form a Gaussian process model,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = GPy.models.GPRegression(X,Y,k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as for the covariance function object, we can find out about the model using the command `print m`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that by default the model includes some observation noise\n",
    "with variance 1. We can see the posterior mean prediction and visualize the marginal posterior variances using `m.plot()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual predictions of the model for a set of points `Xstar`\n",
    "(an $m \\times p$ array) can be computed using `Ystar, Vstar, up95, lo95 = m.predict(Xstar)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) What do you think about this first fit? Does the prior given by the GP seem to be\n",
    "adapted?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Exercise 4 a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) The parameters of the models can be modified using a regular expression matching the parameters names (for example `m.Gaussian_noise = 0.001` or `m.rbf.lengthscale = 0.1`). Change the values of the parameters to obtain a better fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4 b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) As in Section 2, random sample paths from the conditional GP can be obtained using\n",
    "`np.random.multivariate_normal(mu[:,0],C)` where the mean vector and covariance\n",
    "matrix `mu`, `C` are obtained through the predict function `mu, C = m.predict(Xs,full_cov=True)`. Obtain 10 samples from the posterior sample and plot them alongside the data below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4 c) answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance Function Parameter Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen during the lectures, the parameters values can be estimated by maximizing the likelihood of the observations. Since we don’t want one of the variance to become negative during the optimization, we can constrain all parameters to be positive before running the optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.constrain_positive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The warnings are because the parameters are already constrained by default, the software is warning us that they are being reconstrained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can optimize the model using the `m.optimize()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.optimize()\n",
    "m.plot()\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters obtained after optimisation can be compared with the values selected by hand above. As previously, you can modify the kernel used for building the model to investigate its influence on the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 A Running Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll consider a small example with real world data, data giving the pace of all marathons run at the olympics. To load the data use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if in python2, and having an error, uncomment this hack or use python3.\n",
    "#import itertools\n",
    "#if not hasattr(itertools,'zip_longest'):\n",
    "#    itertools.zip_longest = itertools.izip_longest\n",
    "\n",
    "GPy.util.datasets.authorize_download = lambda x: True # prevents requesting authorization for download.\n",
    "data = GPy.util.datasets.olympic_marathon_men()\n",
    "print(data['details'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['X']\n",
    "Y = data['Y']\n",
    "plt.plot(X, Y, 'bx')\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('marathon pace min/km')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Build a Gaussian process model for the olympic data set using a combination of an exponentiated quadratic and a bias covariance function. Fit the covariance function parameters and the noise to the data. Plot the fit and error bars from 1870 to 2030. Do you think the predictions are reasonable? If not why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5 a) answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Fit the same model, but this time intialize the length scale of the exponentiated quadratic to 0.5. What has happened? Which of model has the higher log likelihood, this one or the one from (a)? \n",
    "\n",
    "*Hint:* use `model.log_likelihood()` for computing the log likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5 b) answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Modify your model by including two covariance functions. Intitialize a covariance function with an exponentiated quadratic part, a Matern 3/2 part and a bias covariance. Set the initial lengthscale of the exponentiated quadratic to 80 years, set the initial length scale of the Matern 3/2 to 10 years. Optimize the new model and plot the fit again. How does it compare with the previous model? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5 c) answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Repeat part c) but now initialize both of the covariance functions' lengthscales to 20 years. Check the model parameters, what happens now? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5 d) answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Now model the data with a product of an exponentiated quadratic covariance function and a linear covariance function. Fit the covariance function parameters. Why are the variance parameters of the linear part so small? How could this be fixed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5 e) answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 More Advanced: Uncertainty propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $x$ be a random variable defined over the real numbers, $\\Re$, and $f(\\cdot)$ be a function mapping between the real numbers $\\Re \\rightarrow \\Re$. Uncertainty\n",
    "propagation is the study of the distribution of the random variable $f ( x )$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will see in this section the advantage of using a model when only a few observations of $f$ are available. We consider here the 2-dimensional Branin test function\n",
    "defined over [−5, 10] × [0, 15] and a set of 25 observations as seen in Figure 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Definition of the Branin test function\n",
    "def branin(X):\n",
    "    y = (X[:,1]-5.1/(4*np.pi**2)*X[:,0]**2+5*X[:,0]/np.pi-6)**2\n",
    "    y += 10*(1-1/(8*np.pi))*np.cos(X[:,0])+10\n",
    "    return(y)\n",
    "\n",
    "# Training set defined as a 5*5 grid:\n",
    "xg1 = np.linspace(-5,10,5)\n",
    "xg2 = np.linspace(0,15,5)\n",
    "X = np.zeros((xg1.size * xg2.size,2))\n",
    "for i,x1 in enumerate(xg1):\n",
    "    for j,x2 in enumerate(xg2):\n",
    "        X[i+xg1.size*j,:] = [x1,x2]\n",
    "\n",
    "Y = branin(X)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume here that we are interested in the distribution of $f (U )$ where $U$ is a\n",
    "random variable with uniform distribution over the input space of $f$. We will focus on\n",
    "the computation of two quantities: $E[ f (U )]$ and $P( f (U ) > 200)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Computation of E[ f (U )]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expectation of $f (U )$ is given by $\\int_x f ( x )\\text{d}x$. A basic approach to approximate this\n",
    "integral is to compute the mean of the 25 observations: `np.mean(Y)`. Since the points\n",
    "are distributed on a grid, this can be seen as the approximation of the integral by a\n",
    "rough Riemann sum. The result can be compared with the actual mean of the Branin\n",
    "function which is 54.31."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can fit a GP model and compute the integral of the best predictor\n",
    "by Monte Carlo sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a GP\n",
    "# Create an exponentiated quadratic plus bias covariance function\n",
    "kg = GPy.kern.RBF(input_dim=2, ARD = True)\n",
    "kb = GPy.kern.Bias(input_dim=2)\n",
    "k = kg + kb\n",
    "\n",
    "# Build a GP model\n",
    "m = GPy.models.GPRegression(X,Y,k)\n",
    "\n",
    "# fix the noise variance\n",
    "m.likelihood.variance.fix(1e-5)\n",
    "\n",
    "# Randomize the model and optimize\n",
    "m.randomize()\n",
    "m.optimize()\n",
    "\n",
    "# Plot the resulting approximation to Brainin\n",
    "# Here you get a two-d plot becaue the function is two dimensional.\n",
    "m.plot()\n",
    "\n",
    "# Compute the mean of model prediction on 1e5 Monte Carlo samples\n",
    "Xp = np.random.uniform(size=(int(1e5),2))\n",
    "Xp[:,0] = Xp[:,0]*15-5\n",
    "Xp[:,1] = Xp[:,1]*15\n",
    "mu, var = m.predict(Xp)\n",
    "np.mean(mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Has the approximation of the mean been improved by using the GP model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 6 a) answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) One particular feature of GPs we have not use for now is their prediction variance. Can you use it to define some confidence intervals around the previous result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 6 b) answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Computation of $P( f (U ) > 200)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In various cases it is interesting to look at the probability that $f$ is greater than a given\n",
    "threshold. For example, assume that $f$ is the response of a physical model representing\n",
    "the maximum constraint in a structure depending on some parameters of the system\n",
    "such as Young’s modulus of the material (say $Y$) and the force applied on the structure\n",
    "(say $F$). If the later are uncertain, the probability of failure of the structure is given by\n",
    "$P( f (Y, F ) > \\text{f_max} )$ where $f_\\text{max}$ is the maximum acceptable constraint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) As previously, use the 25 observations to compute a rough estimate of the probability that $f (U ) > 200$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 7 a) answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Compute the probability that the best predictor is greater than the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 7 b) answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Compute some confidence intervals for the previous result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 7 c) answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two values can be compared with the actual value {$P( f (U ) > 200) = 1.23\\times 10^{−2}$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now assume that we have an extra budget of 10 evaluations of f and we want to\n",
    "use these new evaluations to improve the accuracy of the previous result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Given the previous GP model, where is it interesting to add the new observations if we want to improve the accuracy of the estimator and reduce its variance?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Exercise 8 a) answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Can you think about (and implement!) a procedure that updates sequentially the model with new points in order to improve the estimation of $P( f (U ) > 200)$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 8 b) answer"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
